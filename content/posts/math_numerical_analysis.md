---
author: "KnightSnape"
title: "Math Numerical Analysis"
date: "2024-1-25"
description: "The total result of math numerical analysis"
tags: ["math"]
aliases: ["migrate-from-jekyl"]
ShowToc: true
TocOpen: true
weight: 2
---

# 数值分析

## 前言

$\quad$ 数值分析主要考虑问题的数值解方面的内容, 包括但不限于数值算法的构建, 误差传播的影响, 计算复杂度的估算以及高效可靠的计算机的实现. 虽然对于不同的问题有不同的数值模型来构建, 但是他们一般都具有一些共同点:

$\quad$ 数值分析一般会以线性代数, 高等数学, 实分析和泛函分析等内容为基础或者分析工具

$\quad$ 如果一个问题不能直接求解, 尝试考虑一个近似的可解的问题

$\quad$ 稳定性. 这里的稳定性是指模型问题的解对初始数据的敏感度, 也就是说, 解因为初始数据的微小改变而产生的变化, 变化越小越稳定.

数值分析的研究领域非常广泛, 并且由于现代种类交叉学科的发展, 数值分析的边界变得越来越模糊. 但是主要的内容包含如下几个方面:

### 数值线性代数

$\quad$ 主要研究内容主要涉及线性方程组算法研究, 特征值和相关数值算法研究

### 非线性方程组

$\quad$ 非线性方程组的求解一般是以一系列近似的线性问题为工具, 经典的方法包括二分法, 不动点迭代, 最速下降法和牛顿法等

### 逼近理论

$\quad$ 逼近理论主要考虑如何利用简单的函数来逼近复杂函数, 并且如何给出定量误差估计

* [1.数值计算的误差](#1-数值计算的误差)
* [2.内积空间与正交分解](#2-内积空间与正交分解)
* [3.线性方程组问题](#3-线性方程组问题)

  * [3.1.高斯消元法](#31-高斯消元法)
  * [3.2.列主元消去法](#32-列主元消去法)
  * [3.3.矩阵的三角分解法](#33-矩阵的三角分解法)
  * [3.4.矩阵的直接LU分解法](#34-矩阵的直接LU分解法)
  * [3.5.三对角线性方程组的追赶法](#35-三对角线性方程组的追赶法)
  * [3.6.迭代法求解线性方程组](#36-迭代法求解线性方程组)
  * [3.7.雅可比迭代法](#37-雅可比迭代法)
  * [3.8.高斯-赛德尔迭代法](#38-高斯-赛德尔迭代法)
* [4.插值算法](#4-插值算法)

  * [4.1.线性插值](#41-线性插值)
  * [4.2.双线性插值](#42-双线性插值)
  * [4.3.拉格朗日插值](#43-拉格朗日插值)
  * [4.4.牛顿插值](#44-牛顿插值)
* [5.数值微分和积分](#5-数值微分和积分)

  * [5.1.数值微分与计算方法](#51-数值微分与计算方法)
  * [5.2.数值积分与计算方法](#52-数值积分与计算方法)
* [6.数值逼近](#6-数值逼近)

  * [6.1.曲线和曲面的拟合](#61-曲线和曲面的拟合)
  * [6.2.正交多项式与函数逼近](#62-正交多项式与函数逼近)

# 1.数值计算的误差

$\quad$既然直接聊到了误差，那么就直接讲其分类以及来源:

- 模型误差：实际问题和对其进行抽象，简化后得到的数学模型之间存在的误差
- 观测误差：由于精度限制，观察和测量的时候产生的误差
- 舍入误差：计算机字长的限制，所能表示的数只能有有限的位数，后面的部分按照不同的舍入规则舍去产生的误差。
- 截断/方法误差：得不到精确解的数学模型通常用数值方法求近似解，二者之间的误差，通常用有限过程对无穷进行截断，比如：泰勒公式的近似代替如下：

$$
P_n(x) = f(0) + \frac{f`(0)}{1!}x + \frac{f``(0)}{2!}x^2 + ... + \frac{f^{n}(0)}{n!}x^n
$$

则截断误差为:

$$
R_n(x) = f(x) - P_n(x) = \frac{f^{n+1}(\xi)}{(n+1)!}x^{n+1}
$$

其中$\xi \in (0,x)$

<font color=RED>定义1:</font> 设$x$为准确值，$x^*$为$x$的一个近似值，称$E(x^*) = x^* - x$为近似值的绝对误差，简称误差

实际上，准确值$x$通常无法求得甚至未知，因此上面的$E(x^*)$往往也无法求得，只能知道其绝对值的某个上界$\varepsilon(x*) \ge E(x^*) = |x^* - x|$，数值$\varepsilon(x*)$称为$x^*$的(绝对)误差限。

但是一个测量值的精确程度除了与绝对误差限有关外，还和该量的大小有关，由此引入相对误差限：

<font color=RED>定义2:</font> 设$x$为准确值，$x^*$为$x$的一个近似值，称$\frac{E(x^*)}{x} = \frac{x^* - x}{x}$为近似值x^*的相对误差，简记为E_r，$\varepsilon_r(x^*) \ge |E_r(x^*)|$为$x^*$的相对误差限

两种误差限制的关系为:$\varepsilon_r = \frac{\varepsilon}{|x^*|}$

<font color=RED>定义3:</font> 若$x^*$为$x$的近似值，其绝对误差的绝对值不超过某一位数字的半个单位，而该位数字到$x^*$的第一位非零数字共有n位，则称用$x^*$近似$x$时具有$n$位有效数字，简称$x^*$有$n$位有效数字。

有效数字与绝对误差限的关系：

$x$的近似值$x^*$的规格化形式可以写作：

$$
x^* = \pm0.a_1a_2...a_k \times 10^m
$$

其中$m$是整数，$a_i$是0-9中的一个数字且$a_1=0$,则上式具有n位($n \leq k$)有效数字当且仅当$|E| = |x^* - x| \leq 0.5 \times 10^{m - n}$

$x^*$有n位有效数字，则相对误差限:

$$
e_r^* \leq \frac{1}{2a_1} \times 10^{1-n}
$$

反之相对误差限:

$$
e_r^* \leq \frac{1}{2(a_1+1)} \times 10^{1-n}
$$

数值运算的误差估计：

$$
\varepsilon(x_1^* + x_2^*) = \varepsilon(x_1^*) + \varepsilon(x_2^*)
$$

$$
\varepsilon (x_1^*x_2^*) \approx |x_1^*|\varepsilon(x_2^*) + |x_2^*|\varepsilon(x_1^*)
$$

$$
\varepsilon(\frac{x_1^*}{x_2^*}) \approx \frac{|x_1^*|\varepsilon(x_2^*) + |x_2^*|\varepsilon(x_1^*)}{|x_2^*|^2}
$$

一般地，自变量有误差时，计算函数值也产生误差，误差限可利用函数的泰勒展开式进行估计，

$$
\varepsilon(f(x^*)) = f(x) - f(x^*) = f`(x^*)(x - x^*) + \frac{f``(\xi)}{2}(x - x^*)^2
$$

其中$\xi$介于$x$和$x^*$之间

取绝对值并假定$f`(x^*)$和$f``(x^*)$比值不大，忽略$\varepsilon(x^*)$的高阶项，有：

$$
\varepsilon(f(x^*)) \approx |f`(x^*)|\varepsilon(x^*)
$$

多元函数同理

四则运算的稳定性问题：

- 防止大数吃小数(计算机位数有限造成)->求和时从小到大相加，可使和的误差减小
- 做减法时避免相近数相减->使用有理化，三角变换等
- 避免小数作为除数和大数作为乘数

提高算法效率问题：

- 减少运算次数(多项式计算的秦九韶算法)
- 病态问题

<font color=RED>定义4: </font>对数学问题本身如果输入数据有微小扰动，引起输出数据的很大扰动，即病态问题

- 计算函数值$f(x)$，当$x$有扰动，$\delta = x - x^*$，相对误差$\frac{\delta}{x}$，函数值相对误差$\frac{f(x) - f(x^*)}{f(x)}$，相对误差比值为:$\frac{\frac{f(x) - f(x^*)}{f(x)}}{\frac{\delta}{x}} \approx \frac{xf`(x)}{f(x)} = C_p$

其中$C_p$为计算函数值问题的条件数

# 2.内积空间与正交分解

- 矢量的模与赋范空间

向量的模是线性空间中不存在的概念，它就是向量的大小，对于几何而言反映为有向线段的长短，其满足如下性质：

1.正定性:$||\mathbf{a}|| > 0 , ||\mathbf{0}|| = 0$
2.正齐次性:$||\lambda\mathbf{a}||=|\lambda|||\mathbf{a}||$
3.三角不等式:$||\mathbf{a}+\mathbf{b}|| \leq ||a|| + ||b||$

矢量的模运算可以看成是$R^3$空间到$R$的映射，也可以看成一维实数的绝对值在二维和三维空间的推广。我们可以进一步推广到n维向量的模场：

$$
||\mathbf{v}|| = \sqrt{\sum_{i=1}^{n}{x_i^2}}
$$

更一般的，我们不需要定义计算元素“长短”的公式，我们只需要定义计算元素“长短”的运算所满足的条件。比如我们在线性空间中，定义范数(norm)运算，记为$||·||$，使之满足上面的正定性，正齐次性，三角不等式，我们就得到了一个新的代数结构——赋范线性空间。

<font color=RED>矢量的单位化: </font> 单位向量是定义为模长为1的向量,那么任意向量的单位化:

$$
normalize(\mathbf{a}) = \frac{\mathbf{a}}{||\mathbf{a}||}
$$

单位化也常称归一化，规范化。单位化向量在几何和物理中，特指向量的方向。单位向量和原向量线性相关

<font color=RED>赋范线性空间的定义：</font>$V$是数域$F$上的线性空间，映射$||·||:V \to F$满足

1.正定性: $||\mathbf{\xi}|| > 0, ||\mathbf{0}|| = 0$

2.正齐次性：$||k\xi||=|k|||\xi||,k \in F$

3.三角不等式 $||\xi_1 + \xi_2|| \leq ||\xi_1|| + ||\xi_2||$

比如，在线性空间与线性变换中，我们知道所有在闭区间$[a,b]$上连续的函数构成一个线性空间$C[a,b]$，我们可以定义三种常用的范数，使之构成赋范线性空间。那么可以定义函数的范数：

1.$\infty - $范数:$||f(x)||_\infty=max_{a \leq x \leq b}|f(x)|$

2.$1 - $范数:$||f(x)||_1 = \int_{a}^{b}|f(x)|dx$

3.$2 - $范数:$||f(x)||_2 = \sqrt{\int_{a}^{b}{(f(x))^2}dx}$

类比于向量的单位化，与函数范数的定义，我们可以得到函数的单位化：单位函数是范数为1的函数，那么函数的单位化：

$$
normalize[f(x)] = \frac{f(x)}{||f(x)||}
$$

单位函数的反映在函数图像上，保留了原函数的“起伏”形状。也即单位函数与原函数线性相关，而且导函数与原函数都线性相关：

$$
(\frac{f(x)}{||f(x)||})`=\frac{1}{||f(x)||}f`(x)=kf`(x)
$$

$$
\int{\frac{f(x)}{f`(x)}dx}=\frac{1}{||f(x)||}\int{f(x)dx}=k\int{f(x)dx}
$$

范数是模的抽象，是数学上衡量元素“长短”的一般概念，如果是更一般的集合，只要范数运算满足正定性、正齐次性、三角不等式，那么就称它为赋范空间。

- 向量的内积和内积空间

熟悉的内积公式为：

$$
\mathbf{a}·\mathbf{b} = ||\mathbf{a}||||\mathbf{b}||cos<\mathbf{a},\mathbf{b}>
$$

通过内积运算，也建立一个从矢量到标量的映射。矢量内积最重要的几何意义是反应了两矢量的夹角$\alpha$，所以我们也常常使用矢量内积计算两矢量的夹角：

$$
\cos{\alpha} = \frac{\mathbf{a}·\mathbf{b}}{||\mathbf{a}||||\mathbf{b}||}
$$

因为$-1 \leq cos\alpha \leq -1$，所以有
$|\mathbf{a}·\mathbf{b}| \leq ||\mathbf{a}||||\mathbf{b}||$

这个不等式就是柯西-布涅柯夫斯基不等式。

由定义，我们还可以从内积公式中得到向量模的计算公式：

$$
||\mathbf{a}|| = \sqrt{\mathbf{a}·\mathbf{a}}
$$

<font color=RED>内积空间的定义： </font>$V$ 是数域$F$上的线性空间，在$V$上定义内积运算$(\alpha,\beta)$，对$\forall \alpha,\beta \in V$，都有$F$中的一个元素与之对应。它具有以下性质:

- 共轭对称性：$(\alpha,\beta) = \overline{(\beta,\alpha)}$
- 数乘结合律: $(k\alpha,\beta) = k(\alpha,\beta)$
- 加法分配律: $(\alpha + \beta,\gamma) = (\alpha,\gamma) + (\beta,\gamma)$
- 正定性: $(\alpha,\alpha) \geq 0$，当且仅当$\alpha = 0$时$(\alpha,\alpha) = 0$

由定义，内积空间一定是线性空间。

- 正交

<font color=RED>定义： </font>如果$(\alpha,\beta)=0$，称$\alpha,\beta$正交。正交是垂直的推广，只有在内积空间中才有意义。两正交元素是线性无关的。

正交基与标准正交基：在内积空间，一组非零的元素，如果它们两两正交，就称为正交元素组。在$n$维内积空间中，有$n$个两两正交的元素，称为正交基，如果正交基的元素都是单位元素，则称为标准正交基。设$\epsilon_1,\epsilon_2,...,\epsilon_n$是一组正交基，那么：

$$
(\epsilon_i,\epsilon_j) = \left\{
\begin{aligned}
&E_i,when \quad i=j \\
&0,when \quad i \neq j \\
\end{aligned}
\right.
$$

当$E_i=1$时，就是标准正交基。一组基为标准正交基的充要条件是，它的度量矩阵为单位矩阵。在标准正交基下，内积公式可以简化为：

$$
(\alpha,\beta) = \sum_{i=1}^{n}{\sum_{j=1}^{n}{x_iy_i}} = X_TY
$$

<font color=RED>定理(施密特(Schimidt)正交化)： </font>通过$n$维欧式空间中任意一组线性无关的向量组$\{\epsilon_i\}$，都可以找到一组标准的正交基$\{\eta_i\}$

逐个正交化，令$\beta_1 = \epsilon_1$

因为\beta_i是$\{\epsilon_i\}$的线性组合，那么我们可以构造递推公式：

$$
\beta_i = \epsilon_i - \sum_{j=1}^{i-1}{k_j\beta_j},i > 1
$$

其中

$$
k_j = \frac{(\epsilon_{i+1},\beta_i)}{(\beta_i,\beta_i)}
$$

再逐个归一化：

$$
\eta_i = \frac{\beta_i}{|\beta_i|},i=1,2,...,n
$$

# 3.线性方程组问题

## 3.1.高斯消元法

你一定会解简单的线性方程组，比如下面这样的：

$$
2x_1 + 4x_2 - 2x_3 = 2
$$

$$
4x_1 + 9x_2 - 3x_3 = 8
$$

$$
-2x_1 - 3x_2 + 7x_3 = 10
$$

你一定知道，消元的过程就是解方程的过程。对于上面的方程组，
$(1) + (3),(1) \times 2 - (2)$ 可以消掉$x_1$，得到两个方程进一步可以消掉$x_2$ 或 $x_3$，进而解得结果

这个过程可以用矩阵描述，表达的是相同含义。

把方程组左边变量得系数放进矩阵：

$$
A = \begin{bmatrix}
 2 & 4 & -2\\
 4 & 9 & -3\\
 -2 & -3 & 7
\end{bmatrix}
$$

把方程组右边的数字放入矩阵：

$$
b = \begin{bmatrix}
2 \\
8 \\
10
\end{bmatrix}
$$

原方程可以写成：

$$
Ax = b
$$

接下来进行高斯消元得过程，由于等十之间得加减法同时操作等式两边，以上两个矩阵$A,b$的行变换需要同时进行，故把它们放进同一个矩阵：

$$
A` = \begin{bmatrix}
 2 & 4 & -2 & 2\\
 4 & 9 & -3 & 8\\
 -2 & -3 & 7 & 10
\end{bmatrix}
$$

先消掉$(2)$,$(3)$中的$x_1$，也就是把矩阵第2，3行的第一列变成0。可以实现这样的效果：

$$
A^{(1)} = \begin{bmatrix}
 2 & 4 & -2 & 2\\
 0 & 1 & 1 & 4\\
 0 & 1 & 5 & 12
\end{bmatrix}
$$

相当于保持第一个方程不变，使用第一个方程。在第2,3中消去了$x_1$，新的2,3方程如下：

$$
x_2 + x_3 = 4
$$

$$
x_2 + 5x_3 = 12
$$

上面的操作可以看出，把矩阵的一行乘上某个数字，再加/减到另一行，可以看作把方程组中的两个方程拿出来，让他们的等式两边相互加减，就是我们熟悉的消元手法！

## 3.2.列主元消去法

列主元素消去法是为控制舍入误差而提出来的一种算法，列主元素消去法计算基本上能控制舍入误差的影响，其基本思想是：在进行第 $k(k=1,2,…,n-1)$步消元时，从第k列的 akk及其以下的各元素中选取绝对值最大的元素，然后通过行变换将它交换到主元素akk的位置上，再进行消元。

- 优点

高斯消去法从第k步到第k+1步的消元过程，必须满足条件a(kk)不等于零 （kk指下标）。而这个元素 即被称为第k步的主元(素)。显然，高斯消去法是按方程排列的自然顺序产生主元的，这样，一旦出现 计算就归于失败，而且即使a(kk)不等于零 （kk指下标） ，但若其绝对值很小，也将会因用它作除数，引起其他元素的数量级及舍人误差急剧增大，导致最终计算结果不可靠。为了避免在高斯消去法应用中可能出现的这类问题，就发展形成了列主元、全主元等多种消去法。这些方法的基本点在于对高斯消去法的过程作某些技术性修改，全面或局部地选取绝对值最大的元素为主元素，从而构成了相应的主元(素)消去法。列主元(素)消去法以处理简单、相对计算量小的特点，在各类主元消去法中得到最为广泛的应用。

- 算法流程

本算法将消元结果覆盖$A$，乘数$m_{jk}$ ，冲掉$a_{ik}$，解x存放在b内。

1. 对于$k=1,2,3....n$，依次做到第五步
2. 按列选主元，即确定$i_k$，使得

$$
|a_{i_k,k}| = max_{k \leq i \leq n}|a_{ik}|
$$

3. 如果$a_{i_k,k} = 0，则输出无解$
4. 如果$i_k = k$，则转向5，否则换行

$$
a_{kj} \Leftrightarrow a_{i_k,k} (j=k,k+1,...,n)
$$

$$
b_k \Leftrightarrow b_{i_k}
$$

5. 消元过程：

$$
a_{ik} \leftarrow m_{ik} = \frac{a_{ik}}{a_{kk}}, (i=k+1,...,n)
$$

$$
a_{ij} \leftarrow a_{ij} - m_{ik}a_{kj} (i,j=k+1,...,m)
$$

$$
b_i \leftarrow b_i - m_{ik}b_k, (i=k+1,...,n)
$$

6. 如果$a_{00} = 0$，则停机
7. 回代求解

$$
b_n \leftarrow \frac{b_n}{a_{nm}}
$$

$$
b_i \leftarrow \frac{b_i - \sum_{j=i+1}^{n}{a_{ij}b_j}}{a_{ii}},(i=n-1,...,2,1)
$$

8. 输出结果x.

## 3.3.矩阵的三角分解法

假设我们有这样的方程：

$$
2x_1 + x_2 + x_3 = 4
$$

$$
4_x1 + 3x_2 + 3x_3 + x_4 = 11
$$

$$
8x_1 + 7x_2 + 9x_3 + 5x_4 = 29
$$

$$
6x_1 + 7x_2 + 9x_3 + 8x_4 = 30
$$

通过高斯消元法，获得等价问题:

$$
2x_1 + x_2 + x_3 = 4
$$

$$
x_2 + x_3 + x_4 = 3
$$

$$
2x_3 + 2x_4 = 4
$$

$$
2x_4 = 2
$$

这样的消元过程如果写成矩阵相乘的过程，那么如下所示：

$$
L_1A = \begin{bmatrix}
 1 &  &  & \\
 -2 & 1 &  & \\
 -4 &  & 1 &  \\ 
  -3& & & 1
\end{bmatrix}\begin{bmatrix}
 2 & 1 & 1 & 0\\
 0 & 1 & 1 & 1\\
 8 & 7 & 9 & 5 \\ 
 6 & 7 & 9 & 8
\end{bmatrix} = \begin{bmatrix}
 2 & 1 & 1 & 0\\
 0 & 1 & 1 & 1\\
 0 & 3 & 5 & 5 \\ 
 0 & 4 & 6 & 8
\end{bmatrix}
$$

第二次消元意味着

$$
L_2L_1A = \begin{bmatrix}
 1 &  &  & \\
  & 1 &  & \\
  & -3 & 1 &  \\ 
  & -4 & & 1
\end{bmatrix}\begin{bmatrix}
 2 & 1 & 1 & 0\\
 0 & 1 & 1 & 1\\
 8 & 7 & 9 & 5 \\ 
 6 & 7 & 9 & 8
\end{bmatrix} =
\begin{bmatrix}
 2 & 1 & 1 & 0\\
 0 & 1 & 1 & 1\\
 0 & 0 & 2 & 2 \\ 
 0 & 0 & 2 & 4
\end{bmatrix}
$$

第三次消元意味着

$$
L_3L_2L_1A = \begin{bmatrix}
 1 &  &  & \\
  & 1 &  & \\
  &  & 1 &  \\ 
  &  & -1 & 1
\end{bmatrix}
\begin{bmatrix}
 2 & 1 & 1 & 0\\
 0 & 1 & 1 & 1\\
 8 & 7 & 9 & 5 \\ 
 6 & 7 & 9 & 8
\end{bmatrix} = 
\begin{bmatrix}
 2 & 1 & 1 & 0\\
 0 & 1 & 1 & 1\\
 0 & 0 & 2 & 2 \\ 
 0 & 0 & 0 & 2
\end{bmatrix} = U
$$

通过上述过程得到了LU分解的基本形式。

对于n阶方阵A，如果存在n阶单位下三角矩阵L和n阶上三角形矩阵U，使得A=LU，则称其为矩阵A的LU分解，也称为Doolittle分解。

对于Gauss消去法和LU分解法的乘法计算计算量都是在$\frac{n^3}{3}$
 量级，这个就是分解过程的乘法计算量，推导过程可以模拟一下高斯消去法的整体过程，就可以得到如下的等式，

$$
\sum_{k=1}^{n-1}(n-k)(n-k+2) = \frac{1}{3}n^3 + \frac{1}{2}n^2 - \frac{5}{6}n
$$

$$
\sum_{k=1}^{n}(n-k+1)=\frac{n(n+1)}{2}
$$

```
#include<iostream>
using namespace std;

const int n = 3;
//矩阵的ALU分解
void ALU(double a[n][n], double b[n])
{
    double l[n][n] = { 0 };
    double u[n][n] = { 0 };
    int i, r, k;
    //进行U的第一行的赋值
    for (i = 0; i<n; i++)
    {
        u[0][i] = a[0][i];
    }

    //进行L的第一列的赋值
    for (i = 1; i<n; i++)
    {
        l[i][0] = a[i][0] / u[0][0];
    }

    //计算U的剩下的行数和L的剩下的列数
    for (r = 1; r<n; r++)
    {
        for (i = r; i <n; i++)
        {
            double sum1 = 0;
            for (k = 0; k < r; k++)
            {
                sum1 += l[r][k] * u[k][i];
                //cout << "" << r << "" << sum1 << endl;
            }
            u[r][i] = a[r][i] - sum1;
        }


        if(r!=n)
        for(i=r+1;i<n;i++)
        {
            double sum2 = 0;
              for (k = 0; k<r; k++)
            {
                  sum2 += l[i][k] * u[k][r];
            }
                l[i][r] = (a[i][r] - sum2) / u[r][r];
        }

    }

    double y[n] = { 0 };
    y[0] = b[0];
    for (i = 1; i<n; i++)
    {
        double sum3 = 0;
        for (k = 0; k<i; k++)
            sum3 += l[i][k] * y[k];
        y[i] = b[i] - sum3;
    }

    double x[n] = { 0 };
    x[n - 1] = y[n - 1] / u[n - 1][n - 1];
    for (i = n - 2; i >= 0; i--)
    {
        double sum4 = 0;
        for (k = i + 1; k<n; k++)
            sum4 += u[i][k] * x[k];
        x[i] = (y[i] - sum4) / u[i][i];
    }
    for (i = 0; i<n; i++)
        cout << "x[" << i + 1 << "]=" << x[i] << endl;
    return;
}


int main()
{
    double a[3][3] = { 1,2,3,2,5,2,3,1,5 };
    double b[3] = { 14,18,20 };
    ALU(a, b);
    return 0;
}
```

## 3.4.矩阵的直接LU分解法

## 3.5.三对角线性方程组的追赶法

## 3.6.迭代法求解线性方程组

考虑求解线性方程组$Ax=b$，其中$A \in \mathbf{R}^{n \times n}$非奇异。将方程组化为$x = Hx+g$的等价形式，并构造一个收敛到解的迭代序列：$x^{(k+1)}=Hx^{(k)}+g$，其中$\{x^{(k)}\}$为迭代序列，$H$为迭代矩阵。

通过证明可以看到，对于任意初值序列，当$k \to \infty$时，如果$\{x^{(k)}\}有唯一极限$x* \in \mathbf{R}^{n \times n}$，则称$H$为收敛矩阵，且此时$x*$恰为线性方程组$Ax=b$的解。这便是迭代法求解的基本思想，即构造收敛序列来逼近方程组近似的解。

显然，迭代解法的收敛性以及误差估计的问题至关重要，因为这决定了收敛法是否有效。

迭代矩阵$H$为收敛矩阵，当且仅当$H$的谱半径$\rho(H) < 1$

## 3.7.雅可比迭代法

(1) 迭代格式：

任取$x_i^{(0)}(i=1,2,3,...,n)$,依次求解：$x_i^{(k+1)}(k=0,1,...)$

$$
a_{11}x_1^{(k+1)} + a_{12}x_2^{(k)} + a_{13}x_3^{(k)} + ... + a_{1n}x_n^{(k)} = b_1
$$

$$
a_{21}x_1^{(k+1)} + a_{22}x_2^{(k)} + a_{23}x_3^{(k)} + ... + a_{2n}x_n^{(k)} = b_2
$$

$$
a_{n1}x_1^{(k+1)} + a_{n2}x_2^{(k)} + a_{n3}x_3^{(k)} + ... + a_{nn}x_n^{(k)} = b_n
$$

产生迭代格式：

$$
x_1^{(k+1)} = -\frac{1}{a_{11}}(a_{12}x_2^{(k)} + a_{13}x_3^{(k)} + ... + a_{1n}x_n^{(k)}-b_1)
$$

$$
x_2^{(k+1)} = -\frac{1}{a_{21}}(a_{22}x_1^{(k)} + a_{23}x_3^{(k)} + ... + a_{2n}x_n^{(k)}-b_2)
$$

$$
x_n^{(k+1)} = -\frac{1}{a_{21}}(a_{22}x_1^{(k)} + a_{23}x_2^{(k)} + ... + a_{2n}x_{n-1}^{(k)}-b_2)
$$

已知$D = diag(a_{11},...,a_{nn})$ 为对角矩阵，我们称这一迭代格式为求解方程组 $Ax = b$ Jacobi迭代，其相应的矩阵形式为：

$$
x^{(k+1)} = (I - D^{-1}A)x^{(k)} + D^{-1}b
$$

## 3.8.高斯-赛德尔迭代法

# 4.插值算法

## 4.1.线性插值

$\quad$ 单线性插值是在一个方向上进行线性插值,比如X方向。假设我们已知坐标$(x_0,y_0)$ 与$(x_1,y_1)$，要得到$[x_0,x_1]$ 区间内某一位置$x$ 在直线上的值。根据图示，我们得到：

$$
\frac{y - y_0}{x - x_0} = \frac{y_1 - y_0}{x_1 - x_0}
$$

$\quad$ 由于$x$值已经，所以可以从公式得到$y$的值：

$$
y = y_0 + (x - x_0)\frac{y_1 - y_0}{x_1 - x_0} = y_0 + \frac{(x - x_0)y_1 - (x - x_0)y_0}{x_1 - x_0}
$$

$\quad$ 已知$y$求$x$的过程与以上过程相同，只是$x$与$y$要进行交换

## 4.2.双线性插值

$\quad$ 双线性插值是有两个变量的插值函数的单线性插值扩展,器核心思想是在两个方向上分别进行一次线性插值.

$$
f(R_1) \approx \frac{x_2 - x}{x_2 - x_1}f(Q_{11}) + \frac{x - x_1}{x_2 - x_1}f(Q_{21}) \quad where \quad R_1 = (x,y_1)
$$

$$
f(R_2) \approx \frac{x_2 - x}{x_2 - x_1}f(Q_{12}) + \frac{x - x_1}{x_2 - x_1}f(Q_{22}) \quad where \quad R_2 = (x,y_2)
$$

$$
f(P) \approx \frac{y_2 - y}{y_2 - y_1}f(R_1) + \frac{y - y_1}{y_2 - y_1}f(R_2)
$$

## 4.3.拉个朗日插值

$\quad$ 设函数$f(x)$在区别$[a,b]$上有定义，且已经在点$a \leq x_0 < x_1 < x_2 < ... < x_n \leq b$上的函数数值$y_0,y_1,...,y_n$，求构造一个次数不超过$n$的插值多项式

$L_n(x) = a_0 + a_1x + ... + a_nx^n$

使得$L_n(x_i) = y_i(i = 0,1,2,...,n)$成立

$$
n = 1
$$

$f(x)$ 在区间$[x_k,x_{k+1}]$端点处的函数值$f(x_k)$和$f(x_{k+1})$，求构造函数一个线性插值多项式$L_1(x)$使得

$$
L_1(x_k) = y_k = f(x_k)
$$

$$
L_1(x_{k+1}) = y_{k+1} = f(x_{k+1})
$$

成立，由直线的点斜式可以得到：

$$
L_1(x) = y_k + \frac{y_{k+1} - y_k}{x_{k+1} - x_k}(x - x_k) = l_k(x)y_k + l_{k+1}(x)y_{k+1}
$$

其中 $l_k(x) = \frac{x - x_{k+1}}{x_k - x_{k+1}}，l_{k+1}(x) = \frac{x - x_k}{x_{k+1} - x_k}$，这里$l_k(x)$和$l_{k+1}(x)$称作线性插值基函数。

$$
n = 2
$$

已知$f(x)在点x_{k-1},x_k,x_{k+1}$上的函数值

$$
f(x_{k-1}),f(x_k),f(x_{k+1})
$$

，求构造一个二次插值多项式$L_2(x)$，使得

$$
L_2(x_{k-1}) = y_{k-1} = f(x_{k-1})
$$

$$
L_2(x_k) = y_k = f(x_k)
$$

$$
L_2(x_{k+1}) = y_{k+1} = f(x_{k+1})
$$

成立，构造

$$
L_2(x) = y_{k-1}l_{k-1}(x) + y_kl_k(x) + y_{k+1}l_{k+1}(x)
$$

易得

$$
l_{k-1}(x) = \frac{(x - x_k)(x - x_{k+1})}{(x_{k-1} - x_k)(x_{k-1} - x_{k+1})}
$$

$$
l_k(x) = \frac{(x - x_{k-1})(x - x_{k+1})}{(x_k - x_{k+1})(x_k - x_{k+1})}
$$

$$
l_{k+1}(x) = \frac{(x - x_{k-1})(x - x_k)}{(x_{k+1} - x_{k-1})(x_{k+1} - x_k)}
$$

一般情况：

假设$L_n(x) = l_0(x)y_0 + l_1(x)y_1 + ... + l_n(x)y_n$

其中$l_i(x)(i=0,1,2,...,n)$为$n$次多项式，称作$n$次插值基函数，且满足：

$$
l_i(x_j) =
\begin{cases}
1, x_j = x_i \\
0, x_j \neq x_i 
\end{cases} ,(i=0,1,2,...,n)
$$

易得

$$
l_i(x) = \prod_{j = 0,j \neq i}^{n}{\frac{x - x_j}{x_i - x_j}}
$$

## 4.4.牛顿插值

- 差商

设函数$f(x)$，已知其$n$个插值节点为$(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_n,y_n)$，我们定义：

$$
[一阶差商]
$$

$$
f[x_i,x_j] = \frac{f(x_i) - f(x_j)}{x_i - x_j} \quad (i \neq j,x_i \neq x_j)
$$

$$
[二阶差商]
$$

$$
f[x_i,x_j,x_k] = \frac{f[x_i,x_j] - f[x_j,x_k]}{x_i - x_k} \quad (i \neq k)
$$

$$
[n阶差商]
$$

$$
f[x_0,x_1,...,x_n] = \frac{f[x_0,x_1,...,x_{n-1}] - f[x_1,x_2,...,x_n]}{x_0 - x_n}
$$

基于差商的定义，我们可以得到下面的公式：

$$
f(x) = f(x_0) + f[x,x_0](x - x_0)
$$

$$
f[x,x_0] = f[x_0,x_1] + f[x,x_0,x_1](x - x_1)
$$

$$
...
$$

$$
f[x,x_0,x_1,...,x_{n-1}] = f[x_0,x_1,...,x_n] + f[x,x_0,x_1,...,x_n](x - x_n)
$$

不断消去回代可以得到：

$$
f(x) = \sum_{i = 0}^{n}{f[x_0,...,x_i](x-x_0)...(x - x_i)}
$$

这个时候上式有两部分组成，牛顿插值逼近函数$P(x)$，误差函数$R(x)$，去掉就可以得到：

$$
N(x) = f(x_0) + f[x_0,x_1](x - x_0) + ... + f[x_0,...,x_n](x - x_0)...(x - x_{n-1})
$$

# 5.数值微分和积分


## 5.1.数值微分与计算方法

## 5.2.数值积分与计算方法

# 6.数值逼近

## 6.1.曲线和曲面的拟合

我们在这里会介绍下面几种拟合方式：

1. 多项式拟合

2. 样条插值

3. 最小二乘法

- 多项式拟合

$\quad$ 假设有一组数据点，包含$m$个点，为$\{(x_i,y_i)\},i=1,2,..,m$

$\quad$ 我们可以让样本点的分布划成一个$n$次多项式

$$\hat{y} = \sum_{i=0}^{n}{a_ix^{n-i}}$$

$\quad$ 可以看到，$n$ 次多项式有$a_0$到$a_n$这$n+1$个未知的拟合系数，我们要做的就是求出这$n+1$个拟合系数

$\quad$ 我们把样本点的横坐标值$x_i$，带入假定的多项式$\hat{y}$，得$n$次多项式在给定样本点的横坐标处的纵坐标为：

$$\hat{y}_i = a_0x_i^n + a_1x_i^{n-1} + a_2x_i^{n-2} + ... + a_{n-1}x_i + a_n$$

我们需要一个指标来评判所有的$\hat{y}_i$和样本点中的$y_i$相差多少，可以采用残差平方和来表征。

$$\epsilon = \sum_{i=1}^{m}{(\hat{y} - y_i)^2} = \sum_{i=1}^{m}{[(a_0x_i^{n}+a_1x_i^{n-1}+a_2x_i^{n-2}+...+a_{n-1}x_i+a_n)-y_i]^2}$$

$\quad$ 回到求最佳拟合系数的问题，如果有一组拟合系数让$\epsilon$最小，那这组拟合系数我们就可以认为是最好的。

$\quad$ 那么怎么求呢，我们可以对$\epsilon$分别求如下$n+1$次偏导，并令每个偏导为0。(令偏导为0求多元函数的极值，是高中的常见操作了。下式中的$\frac{\partial \epsilon}{\partial a_j}$九二一证明是关于$a_j$的单调增加的一次函数，所以可以证明极值是极小值)

$$\begin{cases}
\frac{\partial \epsilon}{\partial a_0} = 0 \\
\frac{\partial \epsilon}{\partial a_1} = 0 \\
... \\
\frac{\partial \epsilon}{\partial a_n} = 0 \\
\end{cases}$$

其中，上述方程组的某一项，可以展开写成：

$$\frac{\partial \epsilon}{\partial a_j} = \sum_{i=1}^{m}{2x_i^{n-j}[(a_0x_i^n+a_1x_i^{n-1}+a_2x_i^{n-2}+...+a_{n-1}x_i+a_n)-y_i]}=0$$

同理，上述方程组中的其他项也可以展开为包含拟合系数$a_0$到$a_n$的等式。那么，方程组中的$n+1$个灯饰就可以求出这个$n+1$个拟合系数。

这样就可以得到一组让$\epsilon$最小的拟合系数，带回$\hat{y}$的表达式就拟合完啦。

## 6.2.正交多项式与函数逼近


